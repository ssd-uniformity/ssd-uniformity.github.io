<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
    <!-- Google Fonts - Nunito as Avenir Next alternative -->
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@300;400;600;700&display=swap" rel="stylesheet">
    <!-- Add this for better Avenir Next support if you have the license -->
    <link rel="stylesheet" href="https://use.typekit.net/your-project-id.css">
    <!-- Custom CSS -->
    <link href="styles.css" rel="stylesheet">
    <!-- VSCode Syntax Highlighting -->
    <link href="vscode-light-style.css" rel="stylesheet">
    <link rel="icon" type="image/x-icon" href="static/images/sphere.png">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/languages/python.min.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
</head>
<body>
    <!-- Header Section -->
    <header class="header-section" id="overview">
        <div class="container">
            <div class="row">
                <div class="mx-auto text-center">
                    <h1 class="paper-title">Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</h1>
                    <div class="author-line text-center mb-2">
                        <span><a href="https://hc-fang.github.io/" class="text-light author-link">Hung-Chieh&nbsp;Fang<sup>1</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://www.csie.ntu.edu.tw/~htlin/" class="text-light author-link">Hsuan-Tien&nbsp;Lin<sup>1</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://www.cse.cuhk.edu.hk/irwin.king/" class="text-light author-link">Irwin&nbsp;King<sup>2</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://yifeiacc.github.io/" class="text-light author-link">Yifei&nbsp;Zhang<sup>2</sup></a></span>

                        
                    </div>
                    <!-- <div class="project-lead-line mt-2 mb-3 text-center">
                        <span class="text-light"><sup>†</sup> Project Lead</span>
                    </div> -->
                    <!-- Institution Logos Section (Replace the text-based institution line) -->
<!-- Institution Logos Section -->
<div class="institution-logos text-center mb-3">
    <!-- <span class="institution-logo-wrapper"> -->
        <!-- <img src="webpage_assets/logos/meta.png" alt="FAIR, Meta" class="institution-logo" title="FAIR, Meta"> -->
        <!-- <sup>1</sup> -->
        <span class="institution-line" style="margin-right: 1em;"><sup>1</sup>National Taiwan University</span>
        <span class="institution-line"><sup>2</sup>The Chinese University of Hong Kong</span>
    <!-- </span> -->
</div>

                <div class="mt-2 mb-3 text-center">
                    <span class="text-light">ICCV 2025</span>
                </div>

                    <div class="mt-4">
                        <a href="https://arxiv.org/abs/2508.01251" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fas fa-file-alt me-2" style="color: var(--primary-color);"></i>Paper</a>
                        <a href="https://github.com/hc-fang/ssd" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fab fa-github me-2" style="color: var(--primary-color);"></i>Code</a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="container">

        <section id="teaser">
            <div class="col-12 mb-3 text-center">
                <img src="static/images/FedUni_teaser.jpg" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 50%; height: auto;">
            </div>
            <div class="figure-caption">
                <em>
                    By softly separating client representations, SSD encourages them to spread toward different directions, thereby improving global uniformity while preserving local representation expressiveness.
                </em>
            </div> 
        </section>

        <!-- <section id="abstract">
            <h2 class="section-title">Abstract</h2>
                <p>
                    Federated Unsupervised Learning (FUL) aims to learn expressive representations in federated and self-supervised settings. The quality of representations learned in FUL is usually determined by uniformity, a measure of how uniformly representations are distributed in the embedding space. However, existing solutions perform well in achieving intra-client (local) uniformity for local models while failing to achieve inter-client (global) uniformity after aggregation due to non-IID data distributions and the decentralized nature of FUL. To address this issue, we propose Soft Separation and Distillation (SSD), a novel approach that preserves inter-client uniformity by encouraging client representations to spread toward different directions. This design reduces interference during client model aggregation, thereby improving global uniformity while preserving local representation expressiveness. We further enhance this effect by introducing a projector distillation module to address the discrepancy between loss optimization and representation quality. We evaluate SSD in both cross-silo and cross-device federated settings, demonstrating consistent improvements in representation quality and task performance across various training scenarios. Our results highlight the importance of inter-client uniformity in FUL and establish SSD as an effective solution to this challenge.
                </p>
        </section> -->

        <!-- Video Section --> 
        <!-- <section id="video">
            <h2 class="section-title">Video</h2>
            <div class="row justify-content-center">
                <div class="col-lg-8 col-md-10">
                    <div class="ratio ratio-16x9">
                        <iframe src="https://www.youtube.com/embed/bR6_Cq5t7xw?si=gRL4UHImpl6FWQpa" 
                                title="YouTube video player" 
                                frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                                referrerpolicy="strict-origin-when-cross-origin" 
                                allowfullscreen>
                        </iframe>
                    </div>
                </div>
            </div>
        </section> -->
        
        <!-- Motivation Section -->
        <section id="motivation">
            <h2 class="section-title">Motivation</h2>
            <div class="row figure-container my-4">
                <h4>Uniformity in Representation Learning</h4>
                <p>
                    Uniformity is a critical metric in representation learning (<a href="https://arxiv.org/abs/2005.10242" target="_blank">Wang et al., 2020</a>, <a href="https://arxiv.org/abs/2403.00642" target="_blank">Fang et al., 2024</a>), where higher uniformity indicates better preservation of information in the learned representations. In <i>centralized</i> training, jointly optimizing alignment loss, which encourages closeness of similar samples, and uniformity loss, which maximizes preserved information, yields high-quality representations.
                </p>
                <div class="col-12 mb-4">
                    <div class="equation-container" style="background-color: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid var(--primary-color);">
                        $$
                        \mathcal{L}_{\text{align}} = \mathbb{E}_{\mathbf{x}\sim p(\mathbf{x}), \tilde{\mathbf{x}},\tilde{\mathbf{x}}^+ \sim \mathcal{T}(\mathbf{x})} ||\mathbf{z} - \mathbf{z}^+||^2_2, \quad \mathcal{L}_{\text{uniform}} = - \log \mathbb{E}_{\mathbf{z}_i, \mathbf{z}_j \overset{\text{i.i.d}}{\sim} p(\mathbf{z})}\bigl[e^{-t ||\mathbf{z}_i - \mathbf{z}_j||^2_2}\bigr] 
                        $$
                    </div>
                </div>
                <hr>
                <h4>Challenges in Achieving Global Uniformity under Non-IID Data</h4>
                <p>
                    In <i>decentralized</i> training with <b>non-IID data</b>, however, achieving global uniformity is challenging.
                </p>
                <div class="col-12 mb-4">
                    <div class="equation-container" style="background-color: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid var(--primary-color);">
                        $$
                        \mathcal{L}_{\text{uniform}}
                            = - \log \biggl( \underbrace{\sum_{k=1}^K \mathbb{E}_{\mathbf{z}, \mathbf{z}' \overset{\text{i.i.d}}\sim p_k(\mathbf{z})} [e^{-t {||\mathbf{z} - \mathbf{z}'||^2_2}}] }_{\text{intra-client }  \mathcal{L}_{\text{uniform}}^k} \\ + \underbrace{\sum_{i\neq j} \mathbb{E}_{\mathbf{z} \sim p_i(\mathbf{z}), \mathbf{z}'\sim p_j(\mathbf{z})}[e^{-t {||\mathbf{z} - \mathbf{z}'||^2_2}}]}_{\text{inter-client }\mathcal{L}_{\text{uniform}}^{(i, j)}}  \biggr)
                        $$
                    </div>
                </div>
                <p>
                    In this setting, each client $i$ optimizes its local loss function using samples from its own data distribution $p_i$. In homogeneous settings, where client distributions are similar (i.e., $p_i \approx p_j$), optimizing intra-client uniformity naturally promotes good inter-client uniformity. However, in non-IID settings, where client distributions differ significantly, optimizing only intra-client uniformity fails to ensure good inter-client uniformity, leading to suboptimal global representation quality.
                </p>
            </div>
        </section>

        <!-- Method Section -->
        <section id="motivation">
            <h2 class="section-title">Method</h2>
            <div class="row figure-container my-4">
                <p>
                    The core challenge in enhancing inter-client uniformity lies in the federated learning constraint that the server has <u>no access to raw client data or embeddings</u>, making it impossible to directly impose a loss function that operates across different clients.
                </p>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/FedUni.jpg" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 80%; height: auto;">
                </div>
                <!-- <div class="figure-caption">
                    <em>
                        Dimensional-Scaled Regularization encourages client representations to spread toward different directions.
                    </em>
                </div> -->
                <hr>
                <h4>Dimensional-Scaled Regularization</h4>
                <p>
                    Our idea is to assign a specific direction to each client, encouraging their representations to spread toward different directions. Specifically, for each client $k$, we define a dimension-scaling vector $\mathbf{d}_k \in \mathbb{R}^d$, where $d$ is the dimensionality of the embedding space. This vector applies selective scaling to specific dimensions. We then regularize each client's embeddings by encouraging them to move toward their dimension-scaled versions through the following loss:
                    $$
                        \mathcal{L}_{\text{DSR}}^k = \mathbb{E}_{\mathbf{z}\sim p_k(\mathbf{z})}\left[\|\mathbf{z} - \text{stopgrad}(\mathbf{z} \odot \mathbf{d}_k)\|_2^2\right],
                    $$
                    where $\odot$ represents element-wise multiplication, and $\text{stopgrad}(\cdot)$ prevents gradient flow through the scaled target.

                    The effect of DSR is shown below.
                </p>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/dsr_demo.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 50%; height: auto;">
                </div>
                <hr>
                <h4>Projector Distillation</h4>
                <p>
                While DSR effectively enhances uniformity at the <i>embedding</i> level, we observe that this improvement may not fully transfer to the <i>representation</i> level. This discrepancy occurs because the projector $g(\cdot)$ placed between the encoder $f(\cdot)$ and the loss function can absorb much of the optimization effect, as shown below.
                </p>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/projector_distillation.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 80%; height: auto;">
                </div>
                <p>
                    Although removing the projector might seem like a direct solution, prior work (<a href="https://arxiv.org/abs/2212.11491" target="_blank">Gupta et al., 2022</a>, <a href="https://arxiv.org/abs/2403.11391" target="_blank">Xue et al., 2024</a>) has shown that the projector plays a crucial role in separating optimization objectives from representation quality, thereby preventing overfitting to specific self-supervised tasks.
                    To bridge this gap, we introduce Projector Distillation (PD), which explicitly aligns the encoder's representations $\mathbf{h}$ with the projector's embeddings $\mathbf{z}$:
                    $$
                        \mathcal{L}_{\text{distill}}^k = \mathbb{E}_{x\sim p_k(\mathbf{x})}\left[D_{\text{KL}}\left(\sigma(\mathbf{h}) \| \sigma(\mathbf{z})\right)\right],
                    $$
                    This distillation mechanism encourages the encoder to internalize the beneficial structure learned in the embedding space, effectively transferring the improved uniformity from embeddings to representations.
                </p>

            </div>
        </section>

        <!-- Method Section -->
        <section id="motivation">
            <h2 class="section-title">Results</h2>
            <div class="row figure-container my-4">
                <h4>Main Results</h4>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/main_result.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 85%; height: auto;">
                </div>
                <!-- <p>
                    Results on cross-device and cross-silo federated settings.
                </p>  -->
                <br><br>
                <h4>Transfer Learning</h4>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/ood.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 85%; height: auto;">
                </div>
                <p>
                    SSD shows strong transferability to out-of-distribution data, achieving better performance and representation quality. An interesting observation is that FedX, which does not perform well in the main table, shows good representation quality and strong performance in this setting. This implies a correlation between representation quality and transferability.
                </p>
                

                <h4>Soft vs. Hard Separation</h4>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/soft_hard.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 85%; height: auto;">
                </div>
                <p>
                    We adopt a soft separation strategy that regularizes client representations toward a specific direction. In contrast, a more direct approach to maximizing inter-client uniformity is to constrain each client to a distinct, client-specific subspace. As shown in the table, this method indeed achieves the highest uniformity, but at the cost of reduced alignment.
                </p>

                <h4>Why not remove the projector?</h4>
                <div class="col-12 mb-3 text-center">
                    <img src="static/images/projector.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid mx-auto d-block" style="max-width: 85%; height: auto;">
                </div>
                <p>
                    As shown in the table, removing the projector significantly improves uniformity when DSR is applied, highlighting the impact of the separation strategy. However, this also results in a substantial drop in performance. To address this, we propose a strategy that preserves the projector while still maintaining the separation effect.
                </p>
                
                

            </div>
        </section>
         

        <!-- Key Findings Section -->
        <!-- <section id="key-findings">
            <h2 class="section-title">Key Findings</h2>
            
            <div class="row mb-5">
                <div class="col-lg-12">
                    <h4>Dimensional Collapse under Extreme UniDA</h4>
                    <p>Our analysis reveals that dimensional collapse occurs when the source-private ratio is high. We demonstrate this phenomenon through both a toy example for intuitive visualization and rigorous analysis of singular value spectrum.</p>
                    
                    <div class="row figure-container my-4">
                        <div class="col-12 mb-3">
                            <img src="static/images/dc.png" alt="DC" class="img-fluid">
                        </div>
                        <div class="figure-caption">
                            <em>
                                <b>Left</b>: Toy example for visualization, where we sample different numbers of source-private data to study the effect under extreme UniDA. The target representations collapse to a single line under high source-private ratios.
                                <b>Right</b>: Singular value spectrum of target representations under different source-private ratios. Several singular values drop to zero under high source-private ratios.
                            </em>

                        </div>
                    </div>
                </div>
                <div class="col-lg-12">
                    <h4>Degraded Representation Quality Impairs PDM</h4>
                    <p>Our analysis reveals that dimensional collapse occurs when the source-private ratio is high. We demonstrate this phenomenon through both a toy example for intuitive visualization and rigorous analysis of singular value spectrum.</p>
                    
                    <div class="row figure-container my-4">
                        <div class="col-12 mb-3">
                            <img src="static/images/pdm.png" alt="DC" class="img-fluid">
                        </div>
                        <div class="figure-caption">
                            <em>
                                Partial domain matching relies on accurate uncertainty estimation, which could be compromised by poor representation quality.
                            </em>

                        </div>
                        <div class="col-12 mb-3">
                            <img src="static/images/error_analysis.png" alt="DC" class="img-fluid">
                        </div>
                        <div class="figure-caption">
                            <em>
                                Under high source-private ratios, uncertainty estimation must be highly accurate to outperform the source-only baseline, yet the estimation error is substantial. Conversely, under low source-private ratios, moderate accuracy suffices and the estimation error is low.
                            </em>

                        </div>
                    </div>
                </div>
            </div> -->


        <!-- Evaluation Section -->
        <!-- <section id="evaluation">
            <h2 class="section-title">Address Dimensional Collapse without Labels</h2>
            <div class="col-lg-12">
                <h4>De-collapse Techniques from Self-Supervised Learning</h4>
                <p>Dimensional collapse is a well-known issue in self-supervised learning, primarily caused by the contrastive alignment term (<a href="https://arxiv.org/abs/2110.09348" target="_blank">Li et al., 2020</a>). Various methods have been developed to address this problem, including contrastive learning approaches (e.g., AlignUniform, SimCLR), asymmetric models (e.g., SimSiam, BYOL), and redundancy reduction techniques (e.g., VICReg, Barlow Twins). Here, we show how the uniformity term from self-supervised learning can effectively prevent DC in UniDA. We further show that these SSL approaches also work in the paper.</p>
                
                <div class="row figure-container my-4">
                    <div class="col-12 mb-3">
                        <img src="static/images/uniformity.png" alt="DC" class="img-fluid">
                    </div>
                    <div class="figure-caption">
                        <em>
                            The alignment term alone worsens the dimensional collapse, while the uniformity term effectively prevents it. Combining both terms achieves the best performance.
                        </em>
                    </div>
                    <div class="col-12 mb-3">
                        <img src="static/images/result.png" alt="DC" class="img-fluid">
                    </div>
                    <div class="figure-caption">
                        <em>
                            Adding SSL consistently improves PDM performance across the whole spectrum, and substantially outperforms baselines in extreme UniDA scenarios where DC is severe.
                        </em>
                    </div>
                </div>
            </div>
        </section>                     -->
                        

            


<!-- Citations Section -->
<section id="citations" class="citation-section">
    <h2 class="section-title">BibTeX</h2>
            <div class="code-container position-relative" style="margin: 20px 0;">
                <button id="copy-citation-btn" class="btn btn-sm" onclick="copyCitation()" style="position: absolute; top: 10px; right: 10px; z-index: 10;">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="code-block-citation" style="padding: 20px; padding-top: 50px; margin: 0; overflow-x: auto;"><code class="plaintext">@inproceedings{ssd_fang2025,
    title={Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning},
    author={Hung-Chieh Fang and Hsuan-Tien Lin and Irwin King and Yifei Zhang},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2025},
}</code></pre>

    </div>
</section>

    </main>
    <footer class="text-center py-3 mt-5" style="border-top: 1px solid #eee; color: #666; font-size: 0.9em;">
        <div class="container">
            <p class="mb-0">Template modified from <a href="https://jiachenzhu.github.io/DyT/" target="_blank" style="color: #666;">https://jiachenzhu.github.io/DyT/</a></p>
        </div>
    </footer>

    <!-- Bootstrap and JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/js/bootstrap.bundle.min.js"></script>
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
    <!-- Custom JS -->
    <script src="scripts.js"></script>
</body>
</html>