<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
    <!-- Google Fonts - Nunito as Avenir Next alternative -->
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@300;400;600;700&display=swap" rel="stylesheet">
    <!-- Add this for better Avenir Next support if you have the license -->
    <link rel="stylesheet" href="https://use.typekit.net/your-project-id.css">
    <!-- Custom CSS -->
    <link href="styles.css" rel="stylesheet">
    <!-- VSCode Syntax Highlighting -->
    <link href="vscode-light-style.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/languages/python.min.js"></script>
</head>
<body>
    <!-- Header Section -->
    <header class="header-section" id="overview">
        <div class="container">
            <div class="row">
                <div class="mx-auto text-center">
                    <h1 class="paper-title">Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</h1>
                    <div class="author-line text-center mb-2">
                        <span><a href="https://hc-fang.github.io/" class="text-light author-link">Hung-Chieh&nbsp;Fang<sup>1</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://www.csie.ntu.edu.tw/~htlin/" class="text-light author-link">Hsuan-Tien&nbsp;Lin<sup>1</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://www.cse.cuhk.edu.hk/irwin.king/" class="text-light author-link">Irwin&nbsp;King<sup>2</sup></a></span>
                        <span class="mx-2 text-light">•</span>
                        <span><a href="https://yifeiacc.github.io/" class="text-light author-link">Yifei&nbsp;Zhang<sup>2</sup></a></span>

                        
                    </div>
                    <!-- <div class="project-lead-line mt-2 mb-3 text-center">
                        <span class="text-light"><sup>†</sup> Project Lead</span>
                    </div> -->
                    <!-- Institution Logos Section (Replace the text-based institution line) -->
<!-- Institution Logos Section -->
<div class="institution-logos text-center mb-3">
    <!-- <span class="institution-logo-wrapper"> -->
        <!-- <img src="webpage_assets/logos/meta.png" alt="FAIR, Meta" class="institution-logo" title="FAIR, Meta"> -->
        <!-- <sup>1</sup> -->
        <span class="institution-line" style="margin-right: 1em;"><sup>1</sup>National Taiwan University</span>
        <span class="institution-line"><sup>2</sup>The Chinese University of Hong Kong</span>
    <!-- </span> -->
</div>

                <div class="mt-2 mb-3 text-center">
                    <span class="text-light">ICCV 2025</span>
                </div>

                    <div class="mt-4">
                        <a href="https://arxiv.org/abs/2410.11271" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fas fa-file-alt me-2" style="color: var(--primary-color);"></i>Paper</a>
                        <a href="https://github.com/hc-fang/" class="btn btn-light me-2" style="box-shadow: 0 4px 8px rgba(0,0,0,0.1);"><i class="fab fa-github me-2" style="color: var(--primary-color);"></i>Code</a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="container">

        <section id="abstract">
            <h2 class="section-title">WIP</h2>
                <!-- <p>
                    Federated Unsupervised Learning (FUL) aims to learn expressive representations in federated and self-supervised settings. The quality of representations learned in FUL is usually determined by uniformity, a measure of how uniformly representations are distributed in the embedding space. However, existing solutions perform well in achieving intra-client (local) uniformity for local models while failing to achieve inter-client (global) uniformity after aggregation due to non-IID data distributions and the decentralized nature of FUL. To address this issue, we propose Soft Separation and Distillation (SSD), a novel approach that preserves inter-client uniformity by encouraging client representations to spread toward different directions. This design reduces interference during client model aggregation, thereby improving global uniformity while preserving local representation expressiveness. We further enhance this effect by introducing a projector distillation module to address the discrepancy between loss optimization and representation quality. We evaluate SSD in both cross-silo and cross-device federated settings, demonstrating consistent improvements in representation quality and task performance across various training scenarios. Our results highlight the importance of inter-client uniformity in FUL and establish SSD as an effective solution to this challenge.
                </p> -->
        </section>

        <!-- Abstract Section -->
        <!-- <section id="abstract">
            <h2 class="section-title">Abstract</h2>
                <p>
                    Federated Unsupervised Learning (FUL) aims to learn expressive representations in federated and self-supervised settings. The quality of representations learned in FUL is usually determined by uniformity, a measure of how uniformly representations are distributed in the embedding space. However, existing solutions perform well in achieving intra-client (local) uniformity for local models while failing to achieve inter-client (global) uniformity after aggregation due to non-IID data distributions and the decentralized nature of FUL. To address this issue, we propose Soft Separation and Distillation (SSD), a novel approach that preserves inter-client uniformity by encouraging client representations to spread toward different directions. This design reduces interference during client model aggregation, thereby improving global uniformity while preserving local representation expressiveness. We further enhance this effect by introducing a projector distillation module to address the discrepancy between loss optimization and representation quality. We evaluate SSD in both cross-silo and cross-device federated settings, demonstrating consistent improvements in representation quality and task performance across various training scenarios. Our results highlight the importance of inter-client uniformity in FUL and establish SSD as an effective solution to this challenge.
                </p>
        </section> -->

        <!-- Video Section --> 
        <!-- <section id="video">
            <h2 class="section-title">Video</h2>
            <div class="row justify-content-center">
                <div class="col-lg-8 col-md-10">
                    <div class="ratio ratio-16x9">
                        <iframe src="https://www.youtube.com/embed/bR6_Cq5t7xw?si=gRL4UHImpl6FWQpa" 
                                title="YouTube video player" 
                                frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                                referrerpolicy="strict-origin-when-cross-origin" 
                                allowfullscreen>
                        </iframe>
                    </div>
                </div>
            </div>
        </section> -->
        
        <!-- Motivation Section -->
        <!-- <section id="motivation">
            <h2 class="section-title">Motivation</h2>
            <div class="row figure-container my-4">
                <div class="col-12 mb-3">
                    <img src="static/images/motivation.png" alt="Dimensional collapse in extreme UniDA scenarios" class="img-fluid">
                </div>
                <div class="figure-caption">
                    <em><b>Left</b>: Universal domain adaptation addresses both domain shift and category shift. 
                        However, category shift scenarios with high source-private ratios remain under-explored.
                        <b>Right</b>: In such extreme cases, existing partial domain matching (PDM) methods that align source and target data on shared classes fail to outperform the simplest source-only baseline. 
                    </em>
                </div>
            </div>
        </section> -->
         

        <!-- Key Findings Section -->
        <!-- <section id="key-findings">
            <h2 class="section-title">Key Findings</h2>
            
            <div class="row mb-5">
                <div class="col-lg-12">
                    <h4>Dimensional Collapse under Extreme UniDA</h4>
                    <p>Our analysis reveals that dimensional collapse occurs when the source-private ratio is high. We demonstrate this phenomenon through both a toy example for intuitive visualization and rigorous analysis of singular value spectrum.</p>
                    
                    <div class="row figure-container my-4">
                        <div class="col-12 mb-3">
                            <img src="static/images/dc.png" alt="DC" class="img-fluid">
                        </div>
                        <div class="figure-caption">
                            <em>
                                <b>Left</b>: Toy example for visualization, where we sample different numbers of source-private data to study the effect under extreme UniDA. The target representations collapse to a single line under high source-private ratios.
                                <b>Right</b>: Singular value spectrum of target representations under different source-private ratios. Several singular values drop to zero under high source-private ratios.
                            </em>

                        </div>
                    </div>
                </div>
                <div class="col-lg-12">
                    <h4>Degraded Representation Quality Impairs PDM</h4>
                    <p>Our analysis reveals that dimensional collapse occurs when the source-private ratio is high. We demonstrate this phenomenon through both a toy example for intuitive visualization and rigorous analysis of singular value spectrum.</p>
                    
                    <div class="row figure-container my-4">
                        <div class="col-12 mb-3">
                            <img src="static/images/pdm.png" alt="DC" class="img-fluid">
                        </div>
                        <div class="figure-caption">
                            <em>
                                Partial domain matching relies on accurate uncertainty estimation, which could be compromised by poor representation quality.
                            </em>

                        </div>
                        <div class="col-12 mb-3">
                            <img src="static/images/error_analysis.png" alt="DC" class="img-fluid">
                        </div>
                        <div class="figure-caption">
                            <em>
                                Under high source-private ratios, uncertainty estimation must be highly accurate to outperform the source-only baseline, yet the estimation error is substantial. Conversely, under low source-private ratios, moderate accuracy suffices and the estimation error is low.
                            </em>

                        </div>
                    </div>
                </div>
            </div> -->


        <!-- Evaluation Section -->
        <!-- <section id="evaluation">
            <h2 class="section-title">Address Dimensional Collapse without Labels</h2>
            <div class="col-lg-12">
                <h4>De-collapse Techniques from Self-Supervised Learning</h4>
                <p>Dimensional collapse is a well-known issue in self-supervised learning, primarily caused by the contrastive alignment term (<a href="https://arxiv.org/abs/2110.09348" target="_blank">Li et al., 2020</a>). Various methods have been developed to address this problem, including contrastive learning approaches (e.g., AlignUniform, SimCLR), asymmetric models (e.g., SimSiam, BYOL), and redundancy reduction techniques (e.g., VICReg, Barlow Twins). Here, we show how the uniformity term from self-supervised learning can effectively prevent DC in UniDA. We further show that these SSL approaches also work in the paper.</p>
                
                <div class="row figure-container my-4">
                    <div class="col-12 mb-3">
                        <img src="static/images/uniformity.png" alt="DC" class="img-fluid">
                    </div>
                    <div class="figure-caption">
                        <em>
                            The alignment term alone worsens the dimensional collapse, while the uniformity term effectively prevents it. Combining both terms achieves the best performance.
                        </em>
                    </div>
                    <div class="col-12 mb-3">
                        <img src="static/images/result.png" alt="DC" class="img-fluid">
                    </div>
                    <div class="figure-caption">
                        <em>
                            Adding SSL consistently improves PDM performance across the whole spectrum, and substantially outperforms baselines in extreme UniDA scenarios where DC is severe.
                        </em>
                    </div>
                </div>
            </div>
        </section>                     -->
                        

            


<!-- Citations Section -->
<section id="citations" class="citation-section">
    <h2 class="section-title">BibTeX</h2>
            <div class="code-container position-relative" style="margin: 20px 0;">
                <button id="copy-citation-btn" class="btn btn-sm" onclick="copyCitation()" style="position: absolute; top: 10px; right: 10px; z-index: 10;">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="code-block-citation" style="padding: 20px; padding-top: 50px; margin: 0; overflow-x: auto;"><code class="plaintext">@inproceedings{ssd_fang2025,
    title={Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning},
    author={Hung-Chieh Fang and Hsuan-Tien Lin and Irwin King and Yifei Zhang},
    booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},
    year={2025},
}</code></pre>

    </div>
</section>

    </main>
    <footer class="text-center py-3 mt-5" style="border-top: 1px solid #eee; color: #666; font-size: 0.9em;">
        <div class="container">
            <p class="mb-0">Template modified from <a href="https://jiachenzhu.github.io/DyT/" target="_blank" style="color: #666;">https://jiachenzhu.github.io/DyT/</a></p>
        </div>
    </footer>

    <!-- Bootstrap and JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/js/bootstrap.bundle.min.js"></script>
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
    <!-- Custom JS -->
    <script src="scripts.js"></script>
</body>
</html>